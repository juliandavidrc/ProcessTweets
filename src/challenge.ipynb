{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En este archivo puedes escribir lo que estimes conveniente. Te recomendamos detallar tu solución y todas las suposiciones que estás considerando. Aquí puedes ejecutar las funciones que definiste en los otros archivos de la carpeta src, medir el tiempo, memoria, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: memory_profiler in /Users/julianromero/.local/share/virtualenvs/ProcessTweets-raqGfvtZ/lib/python3.11/site-packages (0.61.0)\n",
      "Requirement already satisfied: psutil in /Users/julianromero/.local/share/virtualenvs/ProcessTweets-raqGfvtZ/lib/python3.11/site-packages (from memory_profiler) (5.9.8)\n"
     ]
    }
   ],
   "source": [
    "#! pip install -U memory_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import pyspark\n",
    "import zipfile\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "%load_ext memory_profiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting zip to gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../data\n",
    "#unzip -p ../data/farmers-tweets.json.zip | gzip > farmers-tweets.json.gz\n",
    "unzip -p ../data/farmers-protest-tweets-2021-2-4.json.zip | gzip > farmers-protest-tweets-2021-2-4.json.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading tweets as pyspark object\n",
    "\n",
    "Initially the data is a sample of tweets `./data/farmers-tweets.json`, afterwards we'll load the complete json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "** File to use: ../data/farmers-protest-tweets-2021-2-4.json.gz\n"
     ]
    }
   ],
   "source": [
    "namef = '../data/farmers-protest-tweets-2021-2-4'\n",
    "#namef = 'farmers-tweets'\n",
    "\n",
    "file_path = namef+'.json.gz'\n",
    "print(f\"** File to use: {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first test is perform the data loading over a RDD sprk object.\n",
    "`%memit` magic cell allow us measure memory.\n",
    "\n",
    "*Note: The spark.read.json is running in the .gz file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 174.91 MiB, increment: 0.00 MiB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-----+\n",
      "|created_at|       username|count|\n",
      "+----------+---------------+-----+\n",
      "|2021-02-19|       Preetm91|  267|\n",
      "|2021-02-18|neetuanjle_nitu|  195|\n",
      "|2021-02-17| RaaJVinderkaur|  185|\n",
      "|2021-02-13|MaanDee08215437|  178|\n",
      "|2021-02-12|RanbirS00614606|  176|\n",
      "|2021-02-21|     Surrypuria|  161|\n",
      "|2021-02-18|  rebelpacifist|  153|\n",
      "|2021-02-19|KaurDosanjh1979|  138|\n",
      "|2021-02-23|     Surrypuria|  135|\n",
      "|2021-02-15|         jot__b|  134|\n",
      "+----------+---------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "%memit\n",
    "#Creating SparkSession \n",
    "spark = SparkSession.builder.appName('readJson').getOrCreate()\n",
    "#Read file as pyspark object()   \n",
    "data = spark.read.json(file_path)\n",
    "#transformation and renaming columns steps\n",
    "dfcol = data.withColumn(\"created_at\", data[\"date\"].cast('date'))\\\n",
    "                                .withColumn(\"user_id\", data[\"user.id\"])\\\n",
    "                                .withColumn(\"username\", data[\"user.username\"])\n",
    "\n",
    "df = dfcol.select(col(\"created_at\"), col(\"user_id\"), col(\"username\")).groupBy(\"created_at\", \"username\").count()\n",
    "df.sort(df[\"count\"].desc()).show(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading tweets as pandas df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"../data/farmers-tweets.json\"\n",
    "file_path = '../data/farmers-protest-tweets-2021-2-4.json'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "data = [json.loads(line) for line in open(file_path, 'r')]\n",
    "#print(data[1000:1100])\n",
    "#data[0]['user']['id']\n",
    "df = pd.DataFrame(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      created_at         username  count\n",
      "1981  2021-02-24  MaanDee08215437     57\n",
      "1393  2021-02-20  MangalJ23056160     57\n",
      "2294  2021-02-24   preetysaini321     43\n",
      "261   2021-02-15   Rajesh09Sangam     36\n",
      "713   2021-02-17      Gurpreetd86     32\n",
      "1098  2021-02-17   preetysaini321     30\n",
      "13    2021-02-15      AhluwaliaA2     27\n",
      "368   2021-02-15    akshithepatel     25\n",
      "1085  2021-02-17       nonymous_i     23\n",
      "310   2021-02-15        SonaG2022     22\n"
     ]
    }
   ],
   "source": [
    "#Read file with json.loads()   \n",
    "data = [json.loads(line) for line in open(file_path, 'r')]\n",
    "#convert to dataframe\n",
    "df = pd.DataFrame(data)\n",
    "#transformation and renaming columns steps\n",
    "df[\"created_at\"] = pd.to_datetime(df[\"date\"]).dt.strftime('%Y-%m-%d')\n",
    "#lambda functions to create new columns from user tag\n",
    "df['username'] = df['user'].apply(lambda d: d['username'])\n",
    "df['user_id'] = df['user'].apply(lambda d: d['id'])\n",
    "\n",
    "dfres = df.groupby(['created_at','username'])['id'].count().reset_index(name=\"count\").sort_values(\"count\", ascending=False) \n",
    "\n",
    "print(dfres.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"created_at\"] = pd.to_datetime(df[\"date\"]).dt.strftime('%Y-%m-%d')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
